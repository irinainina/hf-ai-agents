# Урок 2. Что такое LLM

### 1. Определение LLM

* **LLM (Large Language Model)** — модель искусственного интеллекта, обученная на больших объемах текста, способная понимать и генерировать человеческий язык.
* Обучаются на миллионах/миллиардах параметров.
* Чаще всего построены на архитектуре **Transformer** (с 2018 года после успеха BERT).

---

## Архитектура Transformer — простое объяснение с примерами

Transformer — это структура нейросети, которая помогает моделям работать с текстом. Есть три основных типа трансформеров, каждый решает разные задачи:

---

### 1. Энкодер — понимает текст
- Превращает входной текст в числовое представление (вектор), отражающее смысл.
- Пример: **BERT**
- Используется для:
  - Классификации текста (например, определить тему или тональность)
  - Распознавания имен, дат, мест (NER)
  - Семантического поиска

**Пример:**  
Представь, что ты читаешь статью и стараешься понять её суть — энкодер делает примерно то же.

---

### 2. Декодер — создаёт текст
- На основе входных данных постепенно генерирует слова одно за другим.
- Пример: **LLaMA**, GPT
- Используется для:
  - Чат-ботов
  - Генерации текста (рассказы, статьи)
  - Написания кода

**Пример:**  
Как если бы ты рассказывал историю, придумывая её по ходу разговора, слово за словом.

---

### 3. Seq2Seq (энкодер + декодер)
- Сначала понимает вход (энкодер), потом генерирует ответ (декодер).
- Пример: **T5**, **BART**
- Используется для:
  - Перевода с одного языка на другой
  - Создания кратких обзоров (суммаризация)
  - Перефразирования текста

**Пример:**  
Как если бы ты прочитал письмо на иностранном языке, понял его, а затем написал перевод или объяснение.

---

### Почему это важно
- Помогает выбрать подходящую модель под задачу агента.
- Позволяет понять, почему одна модель лучше для понимания, а другая — для генерации.
- Аналогия: разные кухонные приборы для разных задач — резка, смешивание, жарка.

---

### 3. Популярные LLM

* **Deepseek-R1** (DeepSeek)
* **GPT-4** (OpenAI)
* **Llama 3** (Meta)
* **SmolLM2** (Hugging Face)
* **Gemma** (Google)
* **Mistral** (Mistral)

Все эти модели - декодеры. Они нацелены на генерацию текста.

---

### 4. Принцип работы LLM

* Цель: **предсказать следующий токен** на основе предыдущих.
* Токены — подслова, а не целые слова. 
* В английском языке в одном токене в среднем 4 символа.
* Есть специальные токены (**EOS** и др.) для обозначения структуры вывода.
* Процесс генерации **авторегрессионный**: выход одного шага → вход следующего.
* Стратегии выбора токена: **greedy**, **beam search**, др.

---

### 5. Внимание (Attention)

* Механизм, позволяющий модели учитывать важные части входа.
* Ключевые понятия: **context length** (длина контекста) и **attention span** (временной охват внимания).

---

### 6. Подсказки (Prompts)

* Качество ответа сильно зависит от формулировки входа.
* Проектирование подсказок (prompt engineering) — важный навык.

---

### 7. Обучение LLM

* **Предварительное обучение**: self-supervised, masked LM.
* **Дальнейшее дообучение**: supervised fine-tuning для конкретных задач.

---

### 8. Запуск LLM

* Локально (при наличии ресурсов).
* Через облако/API (Hugging Face Hub и др.).

---

### 9. Роль LLM в AI-агентах

* **Мозг агента**: понимание инструкций, поддержание контекста, планирование, выбор инструментов.

---

**Итог:** LLM — ключевая технология в AI-агентах, обеспечивающая языковое понимание и генерацию. Основная задача — предсказание следующего токена, при этом на практике успех зависит от архитектуры, качества обучения и правильных подсказок.
