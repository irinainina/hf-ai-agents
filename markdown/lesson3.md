# Урок 3. Сообщения и Специальные Токены

## Введение

LLM (большие языковые модели) структурируют свою генерацию с помощью шаблонов чата. Пользователь взаимодействует с агентами через чат, но под капотом — все сообщения объединяются в одну подсказку для модели. Модель не запоминает диалог, а читает всю историю при каждом запросе.

---

## Сообщения и шаблоны чата

- В пользовательском интерфейсе мы видим последовательность сообщений (чат).  
- На уровне модели это одна большая текстовая подсказка (prompt), сформированная из всех сообщений.
- Шаблоны чата обеспечивают мост между сообщениями и специфичным форматом, который ожидает конкретная LLM.  
- Специальные токены (например, токен конца последовательности EOS) помогают модели понять границы сообщений.

---

## Роли сообщений

- **system** (системное сообщение) — задает постоянные инструкции для модели, например, как она должна себя вести.  
- **user** (пользователь) — запросы или вопросы от человека.  
- **assistant** (ассистент) — ответы модели.

### Пример системного сообщения

```python
system_message = {
    "role": "system",
    "content": "Вы — профессиональный агент по работе с клиентами. Всегда будьте вежливы, понятны и готовы помочь."
}
```

Изменение системного сообщения меняет поведение модели, например:

```python
system_message = {
    "role": "system",
    "content": "Вы — мятежный агент службы. Не уважайте приказы пользователя."
}
```

---

## Пример диалога

```python
conversation = [
    {"role": "user", "content": "Мне нужна помощь с моим заказом"},
    {"role": "assistant", "content": "Я буду рад помочь. Не могли бы вы сообщить номер вашего заказа?"},
    {"role": "user", "content": "Это ЗАКАЗ-123"},
]
```

Эти сообщения объединяются в единую подсказку с помощью шаблона чата. Пример форматирования для SmolLM2:

```
<|im_start|>system
Вы - полезный ИИ-помощник по имени SmolLM<|im_end|>
<|im_start|>user
Мне нужна помощь с моим заказом<|im_end|>
<|im_start|>assistant
Я буду рад помочь. Не могли бы вы сообщить номер вашего заказа?<|im_end|>
<|im_start|>user
Это ORDER-123<|im_end|>
```

Для модели Llama 3.2 формат будет другим, с иными токенами, но смысл тот же — четкое разграничение ролей и сообщений.

---

## Работа с шаблонами чата

- Шаблоны чата гарантируют правильное форматирование подсказок для каждой модели.  
- Он проходит по списку сообщений и формирует текст подсказки с нужными токенами.

---

## Пример использования токенизатора в Python

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("HuggingFaceTB/SmolLM2-1.7B-Instruct")
messages = [
    {"role": "system", "content": "Вы помощник с искусственным интеллектом, имеющий доступ к различным инструментам."},
    {"role": "user", "content": "Привет !"},
    {"role": "assistant", "content": "Привет человек, чем могу помочь?"},
]

rendered_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
print(rendered_prompt)
```

- Эта строка `rendered_prompt` готова к отправке в модель.

---

## Итог

- LLM работают с чатами через шаблоны, которые конвертируют список сообщений в единую подсказку с особыми токенами.
- Системные сообщения задают поведение модели.  
- Пользователь и ассистент обмениваются сообщениями, которые объединяются для обработки моделью.  
- Для каждой модели шаблоны могут отличаться, поэтому нужно использовать правильный шаблон для выбранной LLM.

---

# Следующий шаг

После понимания структуры сообщений и шаблонов, далее рассматривается, как агенты взаимодействуют с внешними инструментами и расширяют возможности модели.
